
# Data Governance Practice Activity: 

This activity helps to understand data governance principles using a detailed dataset. You will work through data quality checks, privacy protection, metadata management, and visualization. Let's get started!

---

## Step 1: Install Required Tools

Before starting, install the required libraries if you're using Google Colab. Uncomment the following commands and run them:

```python
# !pip install pandas matplotlib
```

---

## Step 2: Load the Dataset

1. Download the dataset from [this link](Dataset/salesdata.csv).
2. Upload it to your Colab or Jupyter Notebook environment.
3. Load and preview the dataset:

```python
import pandas as pd

# Load the dataset
file_path = "sample_data/salesdata.csv"  # Update with your path if needed
data = pd.read_csv(file_path)

# Preview the dataset
print("Dataset Preview:")
print(data.head())
```

### **Expected Output:**
You should see a dataset containing:
- Missing emails (e.g., `None` values).
- Invalid email formats (e.g., `invalid_email`).
- Invalid regions (e.g., `InvalidRegion`, `Unknown`).
- Negative purchase amounts (e.g., `-150.00`).
- Non-numeric purchase amounts (e.g., `"InvalidAmount"`).
- Duplicated rows.

---

## Step 3: Perform Data Quality Checks

### 1. **Check for Missing Values**
```python
print("\nMissing Values:")
print(data.isnull().sum())
```
**Why?**
Missing values indicate gaps in data that may affect downstream processes.

**Expected Output:**
You will find missing values in the `Email` column.

---

### 2. **Check for Invalid Purchase Amounts**
```python
data['PurchaseAmount'] = pd.to_numeric(data['PurchaseAmount'], errors='coerce')
invalid_amounts = data[data['PurchaseAmount'] < 0]
print("\nInvalid Purchase Amounts:")
print(invalid_amounts)
```
**Why?**
Purchase amounts must be non-negative numbers. Negative or non-numeric values are invalid.

**Expected Output:**
Rows with negative values or `NaN` due to non-numeric data.

---

### 3. **Check for Invalid Regions**
```python
allowed_regions = ["North", "East", "South", "West"]
invalid_regions = data[~data["Region"].isin(allowed_regions)]
print("\nInvalid Regions:")
print(invalid_regions)
```
**Why?**
Regions should conform to predefined categories to maintain consistency.

**Expected Output:**
Rows with regions like `InvalidRegion` or `Unknown`.

---

## Step 4: Perform Data Cleaning

### 1. **Remove or Correct Invalid Data**
- Drop rows with missing or invalid emails:
```python
data = data.dropna(subset=["Email"])
```

- Remove rows with invalid regions:
```python
data = data[data["Region"].isin(allowed_regions)]
```

- Replace invalid purchase amounts with `0`:
```python
data["PurchaseAmount"] = data["PurchaseAmount"].apply(lambda x: x if pd.notnull(x) and x >= 0 else 0)
```

**Why?**
Cleaning ensures the dataset is free of inconsistencies and ready for analysis.

---

### 2. **Check for Duplicate Rows**
```python
duplicates = data.duplicated().sum()
print("\nNumber of Duplicate Rows:", duplicates)
data = data.drop_duplicates()
```

**Why?**
Duplicate rows can skew analysis and reporting.

---

## Step 5: Visualize Cleaned Data

### Group Data by Region
```python
region_summary = data.groupby("Region")["PurchaseAmount"].sum()
print(region_summary)
```

### Visualize the Results
```python
import matplotlib.pyplot as plt

region_summary.plot(kind="bar", title="Total Purchases by Region", color=['blue', 'orange', 'green', 'red'])
plt.ylabel("Total Purchase Amount")
plt.xlabel("Region")
plt.show()
```

**Why?**
Visualization helps identify patterns and anomalies across regions.

**Expected Output:**
A bar chart showing total purchases by valid regions.

---

## Step 6: Save the Cleaned Dataset

Save the cleaned dataset for future use:
```python
data.to_csv("cleaned_realistic_data.csv", index=False)
print("\nCleaned Dataset Saved as cleaned_realistic_data.csv")
```

**Why?**
Preserving the cleaned dataset ensures reproducibility and downstream usability.

---

## Detailed Explanation of Observations

1. **Missing Values:**
   - Missing emails were identified and removed because they hinder communication.
   - Missing or incomplete data might result from human errors during data entry or system failures.

2. **Invalid Entries:**
   - Non-numeric purchase amounts (`InvalidAmount`) and negative values (`-150.00`) were cleaned to maintain data integrity.
   - These issues could stem from improper validation during data collection.

3. **Invalid Regions:**
   - Regions outside the predefined list (`InvalidRegion`, `Unknown`) were removed to ensure consistency across the dataset.
   - This might occur due to inconsistent manual inputs or lack of standardized dropdowns in forms.

4. **Duplicates:**
   - Duplicate entries for `CustomerID` were removed to prevent over-counting.
   - Duplicates often occur due to repeated data submissions.

---

## Conclusion

This activity demonstrates:
- How to identify and resolve common data quality issues.
- The importance of maintaining clean, consistent, and valid data for governance.
- Practical steps to ensure data quality through validation, cleaning, and visualization.

**Files:**
1. [Sales Dummy Dataset with Errors (.csv)](Dataset/salesdata.csv)
2. [Colab Notebook for Practice (.ipynb)](Codes/Data_Governance_Practice.ipynb)

# Data Governance Practice Activity: MySQL Edition
---

## Step 1: Create the Database and Table

1. **Create the database:** Use the following command to create the database:
   ```sql
   CREATE DATABASE IF NOT EXISTS salesdata;
   USE salesdata;
   ```

2. **Create the table:** Define the `sales` table to store sales data:
   ```sql
   CREATE TABLE sales (
       CustomerID INT,
       Name VARCHAR(100) NOT NULL,
       Email VARCHAR(255),
       Region VARCHAR(50),
       PurchaseAmount VARCHAR(50)
   );
   ```

---

## Step 2: Insert Sample Data (Including Errors)

Run the following commands to insert the provided dataset into the `sales` table:
```sql
INSERT INTO sales (CustomerID, Name, Email, Region, PurchaseAmount) VALUES
(101, 'Alice', 'alice@example.com', 'North', '250.75'),
(102, 'Bob', 'bob@example.com', 'East', '120.5'),
(103, 'Charlie', 'invalid_email', 'InvalidRegion', '330'),
(104, 'David', 'david@example.com', 'West', '-150'),
(105, 'Eve', 'eve@example.com', 'North', '200'),
(106, 'Frank', 'frank.example.com', 'East', '450'),
(107, 'Grace', 'grace@example.com', 'South', NULL),
(108, 'Helen', NULL, 'West', '310.25'),
(109, 'Ivy', 'ivy@example.com', 'North', '180.75'),
(110, 'Jack', 'jack@example.com', 'Unknown', 'InvalidAmount'),
(105, 'Eve', 'eve@example.com', 'North', '200'),
(103, 'Charlie', 'charlie@example.com', 'South', '330');
```

### Why Errors Were Added
- **Invalid Emails:** Test the ability to detect and clean improper formats.
- **Invalid Regions:** Assess handling of inconsistent categories.
- **Negative and Non-Numeric Purchase Amounts:** Simulate data entry issues.

---

## Step 3: Perform Data Quality Checks

1. **Check for invalid emails:**
   ```sql
   SELECT * FROM sales WHERE Email NOT LIKE '%@%';
   ```

2. **Check for missing emails:**
   ```sql
   SELECT * FROM sales WHERE Email IS NULL;
   ```

3. **Check for invalid regions:**
   ```sql
   SELECT * FROM sales WHERE Region NOT IN ('North', 'East', 'South', 'West');
   ```

4. **Check for invalid purchase amounts:**
   ```sql
   SELECT * FROM sales WHERE PurchaseAmount < 0 OR PurchaseAmount NOT REGEXP '^[0-9]+(\\.[0-9]{1,2})?$';
   ```

---

## Step 4: Clean the Data

1. **Remove duplicates:**
   ```sql
   DELETE FROM sales
   WHERE CustomerID IN (
       SELECT CustomerID
       FROM (
           SELECT CustomerID, COUNT(*) AS count
           FROM sales
           GROUP BY CustomerID
           HAVING count > 1
       ) AS duplicates
   )
   AND NOT (Name = 'Eve' AND Email = 'eve@example.com' AND PurchaseAmount = '200');
   ```

2. **Update invalid emails:**
   ```sql
   UPDATE sales
   SET Email = 'unknown@example.com'
   WHERE Email IS NULL OR Email NOT LIKE '%@%';
   ```

3. **Fix invalid regions:**
   ```sql
   UPDATE sales
   SET Region = 'Unknown'
   WHERE Region NOT IN ('North', 'East', 'South', 'West');
   ```

4. **Fix invalid purchase amounts:**
   ```sql
   UPDATE sales
   SET PurchaseAmount = '0'
   WHERE PurchaseAmount < 0 OR PurchaseAmount NOT REGEXP '^[0-9]+(\\.[0-9]{1,2})?$';
   ```

---

## Step 5: Analyze Cleaned Data

1. **Group by Region and calculate total purchase amounts:**
   ```sql
   SELECT Region, SUM(CAST(PurchaseAmount AS DECIMAL(10, 2))) AS TotalSales
   FROM sales
   GROUP BY Region;
   ```

### Observations:
- Cleaned data ensures accurate aggregation and reporting.
- Regions like `Unknown` indicate entries that require further validation or standardization.

---

## Step 6: Export the Cleaned Data

Save the cleaned dataset for further analysis:
```sql
SELECT * FROM sales INTO OUTFILE '/path/to/cleaned_salesdata.csv'
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
LINES TERMINATED BY '\n';
```

---

## Conclusion

This activity demonstrates:
- Identifying and resolving common data quality issues using SQL.
- Understanding the importance of standardization and validation in data governance.
- Practical experience with data cleaning and analysis.

---

### Additional Resources:
- [MySQL Documentation](https://dev.mysql.com/doc/)



