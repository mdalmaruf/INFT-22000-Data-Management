
# Data Governance Practice Activity: 

This activity helps to understand data governance principles using a detailed dataset. You will work through data quality checks, privacy protection, metadata management, and visualization. Let's get started!

---

## Step 1: Install Required Tools

Before starting, install the required libraries if you're using Google Colab. Uncomment the following commands and run them:

```python
# !pip install pandas matplotlib
```

---

## Step 2: Load the Dataset

1. Download the dataset from [this link](Dataset/salesdata.csv).
2. Upload it to your Colab or Jupyter Notebook environment.
3. Load and preview the dataset:

```python
import pandas as pd

# Load the dataset
file_path = "sample_data/salesdata.csv"  # Update with your path if needed
data = pd.read_csv(file_path)

# Preview the dataset
print("Dataset Preview:")
print(data.head())
```

### **Expected Output:**
You should see a dataset containing:
- Missing emails (e.g., `None` values).
- Invalid email formats (e.g., `invalid_email`).
- Invalid regions (e.g., `InvalidRegion`, `Unknown`).
- Negative purchase amounts (e.g., `-150.00`).
- Non-numeric purchase amounts (e.g., `"InvalidAmount"`).
- Duplicated rows.

---

## Step 3: Perform Data Quality Checks

### 1. **Check for Missing Values**
```python
print("\nMissing Values:")
print(data.isnull().sum())
```
**Why?**
Missing values indicate gaps in data that may affect downstream processes.

**Expected Output:**
You will find missing values in the `Email` column.

---

### 2. **Check for Invalid Purchase Amounts**
```python
data['PurchaseAmount'] = pd.to_numeric(data['PurchaseAmount'], errors='coerce')
invalid_amounts = data[data['PurchaseAmount'] < 0]
print("\nInvalid Purchase Amounts:")
print(invalid_amounts)
```
**Why?**
Purchase amounts must be non-negative numbers. Negative or non-numeric values are invalid.

**Expected Output:**
Rows with negative values or `NaN` due to non-numeric data.

---

### 3. **Check for Invalid Regions**
```python
allowed_regions = ["North", "East", "South", "West"]
invalid_regions = data[~data["Region"].isin(allowed_regions)]
print("\nInvalid Regions:")
print(invalid_regions)
```
**Why?**
Regions should conform to predefined categories to maintain consistency.

**Expected Output:**
Rows with regions like `InvalidRegion` or `Unknown`.

---

## Step 4: Perform Data Cleaning

### 1. **Remove or Correct Invalid Data**
- Drop rows with missing or invalid emails:
```python
data = data.dropna(subset=["Email"])
```

- Remove rows with invalid regions:
```python
data = data[data["Region"].isin(allowed_regions)]
```

- Replace invalid purchase amounts with `0`:
```python
data["PurchaseAmount"] = data["PurchaseAmount"].apply(lambda x: x if pd.notnull(x) and x >= 0 else 0)
```

**Why?**
Cleaning ensures the dataset is free of inconsistencies and ready for analysis.

---

### 2. **Check for Duplicate Rows**
```python
duplicates = data.duplicated().sum()
print("\nNumber of Duplicate Rows:", duplicates)
data = data.drop_duplicates()
```

**Why?**
Duplicate rows can skew analysis and reporting.

---

## Step 5: Visualize Cleaned Data

### Group Data by Region
```python
region_summary = data.groupby("Region")["PurchaseAmount"].sum()
print(region_summary)
```

### Visualize the Results
```python
import matplotlib.pyplot as plt

region_summary.plot(kind="bar", title="Total Purchases by Region", color=['blue', 'orange', 'green', 'red'])
plt.ylabel("Total Purchase Amount")
plt.xlabel("Region")
plt.show()
```

**Why?**
Visualization helps identify patterns and anomalies across regions.

**Expected Output:**
A bar chart showing total purchases by valid regions.

---

## Step 6: Save the Cleaned Dataset

Save the cleaned dataset for future use:
```python
data.to_csv("cleaned_realistic_data.csv", index=False)
print("\nCleaned Dataset Saved as cleaned_realistic_data.csv")
```

**Why?**
Preserving the cleaned dataset ensures reproducibility and downstream usability.

---

## Detailed Explanation of Observations

1. **Missing Values:**
   - Missing emails were identified and removed because they hinder communication.
   - Missing or incomplete data might result from human errors during data entry or system failures.

2. **Invalid Entries:**
   - Non-numeric purchase amounts (`InvalidAmount`) and negative values (`-150.00`) were cleaned to maintain data integrity.
   - These issues could stem from improper validation during data collection.

3. **Invalid Regions:**
   - Regions outside the predefined list (`InvalidRegion`, `Unknown`) were removed to ensure consistency across the dataset.
   - This might occur due to inconsistent manual inputs or lack of standardized dropdowns in forms.

4. **Duplicates:**
   - Duplicate entries for `CustomerID` were removed to prevent over-counting.
   - Duplicates often occur due to repeated data submissions.

---

## Conclusion

This activity demonstrates:
- How to identify and resolve common data quality issues.
- The importance of maintaining clean, consistent, and valid data for governance.
- Practical steps to ensure data quality through validation, cleaning, and visualization.

---

**Files:**
1. [Sales Dummy Dataset with Errors (.csv)](Dataset/salesdata.csv)
2. [Colab Notebook for Practice (.ipynb)](Codes/Data-Governance-Practice.ipynb)


