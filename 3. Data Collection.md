# Data Collection in the Wild: Metadata and Data Crawling

### Objectives:

- Understand different types of metadata
- Learn why metadata is important
- Explore methods of data collection
- Implement basic data crawling techniques

**Example Scenario:** Imagine you are building a dataset for analyzing social media trends. You need metadata about posts (timestamps, authors, hashtags) and the actual post content. How do you collect this data ethically and efficiently?

## What is Metadata?

### Definition:

Metadata is "data about data," providing context and structure.

### Types of Metadata:

- **Descriptive**: Provides details like title, author, or creation date (e.g., Image title: "sunset.jpg")
- **Structural**: Defines how different data elements relate (e.g., a book index)
- **Administrative**: Includes rights management, ownership, and licensing
- **Provenance**: Tracks changes and sources of data
- **Statistical**: Specifies formats, data types, and summary statistics

**Example:**

- **Image Metadata:**
  - Title: "sunset.jpg"
  - Camera: Canon EOS 5D
  - GPS Coordinates: 34.0522° N, 118.2437° W
- **File Metadata:**
  - File size: 3MB
  - Created: 2024-01-10
  - Owner: Admin
 
```python
# !pip install easyocr
import easyocr

# Create an OCR reader object
reader = easyocr.Reader(['en'])

# Read text from an image
result = reader.readtext('image.png')

# Print the extracted text
for detection in result:
    print(detection[1])
```

## Why is Metadata Important?

### Purpose and Value:

- Enables **efficient search and retrieval** of data
- Ensures **data integrity and tracking**
- Helps with **compliance regulations** (e.g., GDPR, HIPAA)
- Supports **machine learning** by providing structured datasets

### Real-World Use Cases:

- **Google Search:** Uses metadata (title, description) to rank results
- **Social Media:** Uses metadata to suggest content
- **Data Governance:** Ensures that financial transactions comply with regulations

## Methods of Data Collection

### Key Techniques:

1. **Observations** – Data collected from cameras, sensors, or logs
2. **Surveys** – User-submitted responses (e.g., Google Forms)
3. **Open Data Sources** – Public databases (e.g., Kaggle, World Bank)
4. **Web Scraping & Crawling** – Automated data extraction from websites
5. **Synthetic Data** – AI-generated data to augment real-world datasets

**Example:**

- **NASA's Earth Observing System** collects satellite imagery
- **Twitter API** allows users to collect public tweet data
- **Google Forms** gathers survey responses

## Introduction to Web Scraping & Crawling

### Definitions:

- **Web Scraping:** Extracting specific data from websites
- **Web Crawling:** Automatically navigating multiple web pages to gather data

### Ethical Considerations:

- Check [robots.txt](https://moz.com/learn/seo/robotstxt) to respect site policies
- Follow **terms of service** to avoid legal issues
- Use **throttling** to prevent overloading servers

## Metadata Extraction Example using Python

### Example: Extracting Metadata from a Web Page

**URL:** `https://example.com`

```python
import requests
from bs4 import BeautifulSoup

url = "https://example.com"
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")

# Extract metadata
title = soup.find("title").text
description = soup.find("meta", attrs={"name": "description"})

print("Title:", title)
print("Description:", description["content"] if description else "No description")
```
**Expected Output:**

```
Title: Example Domain
Description: No description
```

```python
# Extract h1 text
h1_text = soup.find("h1").text

# Extract all paragraph (<p>) texts
paragraphs = [p.text for p in soup.find_all("p")]

print("H1:", h1_text)
print("Paragraphs:", paragraphs)
```




## Web Crawling Example with Python

### Example: Scraping Quotes Website

**URL:** `http://quotes.toscrape.com`

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = ["http://quotes.toscrape.com"]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                'text': quote.css("span.text::text").get(),
                'author': quote.css("small.author::text").get()
            }
# Terminal run:
## if you run the python file from terminal: scrapy runspider quotes_spider.py
## to save in .json form: scrapy runspider quotes_spider.py -o quotes.json

```

```python
#!pip install scrapy
# run in Jupyter or colab
import scrapy
from scrapy.crawler import CrawlerProcess

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = ["http://quotes.toscrape.com"]

    def parse(self, response):
        for quote in response.css("div.quote"):
            text = quote.css("span.text::text").get()
            author = quote.css("small.author::text").get()
            tags = quote.css("div.tags a.tag::text").getall()

            print(f"Quote: {text}")
            print(f"Author: {author}")
            print(f"Tags: {tags}\n")

# Run Scrapy inside Jupyter Notebook
process = CrawlerProcess(settings={
    "LOG_LEVEL": "ERROR",  # Set to DEBUG for full logs
})

process.crawl(QuotesSpider)
process.start()

```

**Expected Output:**

```
{'text': '“The only limit to our realization of tomorrow is our doubts of today.”', 'author': 'Franklin D. Roosevelt'}
```

## Storing Extracted Data in SQL

### Example: Storing Scraped Data in PostgreSQL

```sql
CREATE TABLE scraped_data (
    id SERIAL PRIMARY KEY,
    text TEXT,
    author VARCHAR(255)
);

INSERT INTO scraped_data (text, author) VALUES 
('The only limit to our realization of tomorrow is our doubts of today.', 'Franklin D. Roosevelt');
```

**Expected Output:**

```
1 | "The only limit to our realization of tomorrow is our doubts of today." | "Franklin D. Roosevelt"
```

## Developing a Respectful Web Crawler

### Best Practices:

1. **Check robots.txt** before scraping
2. **Throttle requests** to prevent excessive server load
3. **Use headers and proxies** to mimic human browsing
4. **Store data efficiently** in databases or cloud storage

**Example: Checking robots.txt in Python**

```python
import requests

url = "https://www.google.com/robots.txt"
response = requests.get(url)
print(response.text)  # This should return Google's actual robots.txt file.
```

**Expected Output:**

```
User-agent: *
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
Disallow: /sdch
Disallow: /groups
```

## Summary & Key Takeaways

- **Metadata** is crucial for data management
- **Different data collection methods** serve various purposes
- **Web scraping & crawling** can automate data collection
- **Ethics and compliance** are essential in data collection
- **Python & SQL** enable effective data extraction and storage

### Next Steps:

- **Try extracting metadata from different websites**
- **Explore REST APIs for structured data collection**
- **Work on a real-world data project using web scraping**

---


## REST API Data Collection Example

### Example: Fetching Data from OpenWeatherMap API

**API:** [OpenWeatherMap](https://openweathermap.org/api)

**URL:** `http://api.openweathermap.org/data/2.5/weather?q=London&appid=YOUR_API_KEY`

**Python Code:**

```python
import requests

api_key = "YOUR_API_KEY"
city = "London"
url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}"

response = requests.get(url)
data = response.json()

print(f"City: {data['name']}")
print(f"Temperature: {data['main']['temp']}K")
print(f"Weather: {data['weather'][0]['description']}")
```

**Expected Output:**

```
City: London
Temperature: 277.14K
Weather: clear sky
```

## Data Quality Assessment

### Checking Data Quality

- **Completeness:** Are there missing values?
- **Accuracy:** Is the data correct?
- **Consistency:** Are formats consistent?
- **Timeliness:** Is the data up-to-date?

### Example: Data Observation

```python
import requests
import pandas as pd
import time

# Define API key and base URL
API_KEY = ""
BASE_URL = "http://api.openweathermap.org/data/2.5/weather"

# List of cities to collect data for
cities = ["London", "New York", "Paris", "Toronto", "Berlin"]

# Function to fetch weather data
def fetch_weather(city):
    url = f"{BASE_URL}?q={city}&appid={API_KEY}&units=metric"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        return {
            "City": data["name"],
            "Temperature (°C)": data["main"]["temp"],
            "Weather": data["weather"][0]["description"],
            "Humidity (%)": data["main"]["humidity"],
            "Wind Speed (m/s)": data["wind"]["speed"]
        }
    else:
        print(f"Failed to fetch data for {city}")
        return None

# Fetch data for all cities
weather_data = [fetch_weather(city) for city in cities if fetch_weather(city) is not None]

# Convert to DataFrame
df = pd.DataFrame(weather_data)
df.head()
```

## Database Creation & Normalization

### Example: Creating a Weather Data Table

```sql
CREATE TABLE WeatherData (
    id SERIAL PRIMARY KEY,
    city VARCHAR(50),
    temperature FLOAT,
    weather VARCHAR(255),
    humidity INT,
    wind_speed FLOAT
);

INSERT INTO WeatherData (city, temperature, weather, humidity, wind_speed) VALUES
('London', 3.95, 'broken clouds', 84, 5.14),
('New York', 2.98, 'mist', 91, 5.14),
('Paris', 4.05, 'overcast clouds', 79, 6.69),
('Toronto', 0.59, 'overcast clouds', 81, 11.32),
('Berlin', 3.01, 'overcast clouds', 83, 3.13);
```
## Understanding Relationships & Normalization

### Types of Relationships

1. **One-to-One (1:1):** Each entity in Table A is linked to one entity in Table B.
   - Example: A country has **one** capital.
2. **One-to-Many (1****:M****):** One entity in Table A is linked to multiple entities in Table B.
   - Example: A city has **many** weather records over time.
3. **Many-to-Many (M****:M****):** Multiple entities in Table A relate to multiple entities in Table B.
   - Example: Weather conditions can be observed in multiple cities, and a city can experience different weather conditions.

### Example ERD:

```
+------------+        +----------------+
| Cities     |        | WeatherData    |
+------------+        +----------------+
| City_ID    | <----> | City_ID        |
| Name       |        | Temperature    |
| Country    |        | wind_speed    |
+------------+        +----------------+
```

### Data Normalization

- **First Normal Form (1NF):** Ensure atomic values; no repeating groups.
- **Second Normal Form (2NF):** Remove partial dependencies.
- **Third Normal Form (3NF):** Ensure no transitive dependencies.

**Example:** Instead of storing repeated city names, we split them into separate tables:

```sql
CREATE TABLE Cities (
    city_id SERIAL PRIMARY KEY,
    name VARCHAR(50),
    country VARCHAR(50)
);

CREATE TABLE WeatherData (
    id SERIAL PRIMARY KEY,
    city_id INT REFERENCES Cities(city_id),
    temperature FLOAT,
    weather VARCHAR(255),
    humidity INT,
    wind_speed FLOAT
);

```

```bash
INSERT INTO Cities (name, country) VALUES
('London', 'UK'),
('New York', 'USA'),
('Paris', 'France'),
('Toronto', 'Canada'),
('Berlin', 'Germany');

INSERT INTO WeatherData (city_id, temperature, weather, humidity, wind_speed) VALUES
(1, 3.95, 'broken clouds', 84, 5.14),
(2, 2.98, 'mist', 91, 5.14),
(3, 4.05, 'overcast clouds', 79, 6.69),
(4, 0.59, 'overcast clouds', 81, 11.32),
(5, 3.01, 'overcast clouds', 83, 3.13);

```

## Summary & Key Takeaways

- **REST APIs** like OpenWeatherMap provide structured weather data.
- **Data quality checks** ensure meaningful analysis.
- **Databases & normalization** help efficiently store and structure data.
- **Relationships** (1:1, 1\:M, M\:M) define how entities connect in a database.

### Next Steps:

- **Experiment with other public APIs (e.g., COVID-19 API, JSONPlaceholder API).**
- **Practice data normalization and entity relationship modeling.**



