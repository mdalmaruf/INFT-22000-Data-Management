# Data Collection in the Wild: Metadata and Data Crawling

## Introduction to Data Collection in the Wild

### Objectives:

- Understand different types of metadata
- Learn why metadata is important
- Explore methods of data collection
- Implement basic data crawling techniques

**Example Scenario:** Imagine you are building a dataset for analyzing social media trends. You need metadata about posts (timestamps, authors, hashtags) and the actual post content. How do you collect this data ethically and efficiently?

## What is Metadata?

### Definition:

Metadata is "data about data," providing context and structure.

### Types of Metadata:

- **Descriptive**: Provides details like title, author, or creation date (e.g., Image title: "sunset.jpg")
- **Structural**: Defines how different data elements relate (e.g., a book index)
- **Administrative**: Includes rights management, ownership, and licensing
- **Provenance**: Tracks changes and sources of data
- **Statistical**: Specifies formats, data types, and summary statistics

**Example:**

- **Image Metadata:**
  - Title: "sunset.jpg"
  - Camera: Canon EOS 5D
  - GPS Coordinates: 34.0522° N, 118.2437° W
- **File Metadata:**
  - File size: 3MB
  - Created: 2024-01-10
  - Owner: Admin

## Why is Metadata Important?

### Purpose and Value:

- Enables **efficient search and retrieval** of data
- Ensures **data integrity and tracking**
- Helps with **compliance regulations** (e.g., GDPR, HIPAA)
- Supports **machine learning** by providing structured datasets

### Real-World Use Cases:

- **Google Search:** Uses metadata (title, description) to rank results
- **Social Media:** Uses metadata to suggest content
- **Data Governance:** Ensures that financial transactions comply with regulations

## Methods of Data Collection

### Key Techniques:

1. **Observations** – Data collected from cameras, sensors, or logs
2. **Surveys** – User-submitted responses (e.g., Google Forms)
3. **Open Data Sources** – Public databases (e.g., Kaggle, World Bank)
4. **Web Scraping & Crawling** – Automated data extraction from websites
5. **Synthetic Data** – AI-generated data to augment real-world datasets

**Example:**

- **NASA's Earth Observing System** collects satellite imagery
- **Twitter API** allows users to collect public tweet data
- **Google Forms** gathers survey responses

## Introduction to Web Scraping & Crawling

### Definitions:

- **Web Scraping:** Extracting specific data from websites
- **Web Crawling:** Automatically navigating multiple web pages to gather data

### Ethical Considerations:

- Check **robots.txt** to respect site policies
- Follow **terms of service** to avoid legal issues
- Use **throttling** to prevent overloading servers

## Metadata Extraction Example using Python

### Example: Extracting Metadata from a Web Page

**URL:** `https://example.com`

```python
import requests
from bs4 import BeautifulSoup

url = "https://example.com"
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")

# Extract metadata
title = soup.find("title").text
description = soup.find("meta", attrs={"name": "description"})

print("Title:", title)
print("Description:", description["content"] if description else "No description")
```

**Expected Output:**

```
Title: Example Domain
Description: No description
```

## Web Crawling Example with Python

### Example: Scraping Quotes Website

**URL:** `http://quotes.toscrape.com`

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = ["http://quotes.toscrape.com"]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                'text': quote.css("span.text::text").get(),
                'author': quote.css("small.author::text").get()
            }
```

**Expected Output:**

```
{'text': '“The only limit to our realization of tomorrow is our doubts of today.”', 'author': 'Franklin D. Roosevelt'}
```

## Storing Extracted Data in SQL

### Example: Storing Scraped Data in PostgreSQL

```sql
CREATE TABLE scraped_data (
    id SERIAL PRIMARY KEY,
    text TEXT,
    author VARCHAR(255)
);

INSERT INTO scraped_data (text, author) VALUES 
('The only limit to our realization of tomorrow is our doubts of today.', 'Franklin D. Roosevelt');
```

**Expected Output:**

```
1 | "The only limit to our realization of tomorrow is our doubts of today." | "Franklin D. Roosevelt"
```

## Developing a Respectful Web Crawler

### Best Practices:

1. **Check robots.txt** before scraping
2. **Throttle requests** to prevent excessive server load
3. **Use headers and proxies** to mimic human browsing
4. **Store data efficiently** in databases or cloud storage

**Example: Checking robots.txt in Python**

```python
import requests

url = "https://example.com/robots.txt"
response = requests.get(url)
print(response.text)
```

**Expected Output:**

```
User-agent: *
Disallow: /admin/
```

## Summary & Key Takeaways

- **Metadata** is crucial for data management
- **Different data collection methods** serve various purposes
- **Web scraping & crawling** can automate data collection
- **Ethics and compliance** are essential in data collection
- **Python & SQL** enable effective data extraction and storage

### Next Steps:

- **Try extracting metadata from different websites**
- **Explore REST APIs for structured data collection**
- **Work on a real-world data project using web scraping**

