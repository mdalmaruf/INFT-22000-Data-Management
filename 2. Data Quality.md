
# **Data Quality**
- Data Quality refers to the accuracy, completeness, reliability, and relevance of data. It ensures that data is fit for consumption by organizations and decision-makers.

- Data quality is a crucial aspect of data analysis and machine learning. Poor data quality can lead to incorrect analysis, misleading insights, and unreliable decision-making. In this lab, we will walk through a **step-by-step process** to analyze and improve data quality. We will cover:
    - Identifying missing values
    - Handling missing values
    - Detecting and removing duplicates
    - Understanding correlation through heatmaps
    - Standardizing and validating data
## **Importance**
- High-quality data is essential for accurate decision-making and business efficiency.
- Poor data quality leads to errors, inefficiencies, and compliance risks.

## **Key Challenges**
- Lack of standardized definitions and governance.
- Data silos causing inconsistencies.
- Inadequate documentation and training.
## **Table of Contents**
In this Lab, we will apply the following methods for **data cleaning**:
1. [Correct Data Format](#correct-data-format)
2. [Data Standardization](#data-standardization)
3. [Data Normalization (centering/scaling)](#data-normalization)
4. [Count Unique Values](#count-unique-values)
5. [Handle Inconsistent Data](#handle-inconsistent-data)
6. [Identify Missing Values](#identify-missing-values)
7. [Deal with Missing Values](#deal-missing-values)

---
# **Practice Example: Data Quality Analysis - Step-by-Step Tutorial**
We will use three datasets for this lab: 
- **auto.csv**: Contains vehicle-related data
- **Salaries.csv**: Represents salary information of employees
- **flights.csv**: Contains flight details

## **Step 1: Import Required Libraries**
Before starting, we need to import the necessary Python libraries.

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
print("Libraries imported successfully!")
```

### **Explanation**
- `pandas`: Handles data manipulation and processing.
- `seaborn`: Used for visualization.
- `matplotlib`: Helps in plotting graphs.
- `numpy`: Supports numerical computations.

## **Step 2: Load the Datasets**
We load the three datasets to explore their structure.

```python
auto_df = pd.read_csv("/mnt/data/auto.csv")
salaries_df = pd.read_csv("/mnt/data/Salaries.csv")
flights_df = pd.read_csv("/mnt/data/flights.csv")
print("Datasets loaded successfully!")
```

```python
# load a single dataset
df = pd.read_csv("Salaries.csv")

# Check the number of records (rows, columns)
df.shape

# Check the total number of elements
df.size

# Display column names
df.columns

# Check the data types of each column
df.dtypes

# Check a specific column's data type
df.phd.dtype

```

# **Part 1: Correct Data Format**

## **What is Data Format Correction?**
Before we analyze the dataset, we must ensure that **each column has the correct data type**:

- **Numerical values** should be of type **float or int**.
- **Categorical values** should be of type **object (string).**

---

## **Checking Data Types**

```python
df = pd.read_csv('auto.csv')

# View the first few rows of the dataset
df.head()

# List data types for each column
df.dtypes
```
As we can see above, some columns are not of the correct data type. Numerical variables should have type 'float' or 'int', and variables with strings such as categories should have type 'object'. For example, 'bore' and 'stroke' variables are numerical values that describe the engines, so we should expect them to be of the type 'float' or 'int'; however, they are shown as type 'object'. We have to convert data types into a proper format for each column using the "astype()" method.

## **Fixing Data Types in a Dataset**

### **Why is This Important?**
Some columns might be stored as **objects (strings)** instead of **numerical values**, which can cause errors in computations.

For example, in the **Auto dataset**:
- `'bore'` and `'stroke'` describe engine properties, so they should be **float**.
- `'normalized-losses'` should be **int**.
- `'price'` should be **float**.

---

### **Fixing Data Types in Pandas**

```python
# Convert data types to their proper formats
df[["bore", "stroke"]] = df[["bore", "stroke"]].astype("float")
df[["normalized-losses"]] = df[["normalized-losses"]].astype("int")
df[["price"]] = df[["price"]].astype("float")
df[["peak-rpm"]] = df[["peak-rpm"]].astype("float")

# Check the columns after conversion
df.dtypes

```

# **Part 2: Data Standardization**
Data is usually collected from different agencies with different formats.
(Data Standardization is also a term for a particular type of data normalization, where we subtract the mean and divide by the standard deviation)
## **What is Standardization?**
Standardization is the process of transforming data into a common format which allows the researcher to make the meaningful comparison

## **Example**
Transform mpg to L/100km:
In our dataset, the fuel consumption columns "city-mpg" and "highway-mpg" are represented by mpg (miles per gallon) unit. Assume we are developing an application in a country that accept the fuel consumption with L/100km standard

We will need to apply data transformation to transform mpg into L/100km?

The formula for unit conversion is

L/100km = 235 / mpg

We can do many mathematical operations directly in Pandas.

```python
# Convert mpg to L/100km by mathematical operation (235 divided by mpg)
df['city-L/100km'] = 235/df["city-mpg"]

# check your transformed data 
df.head()
```
---
# **Part 3: Data Normalization**
## **Why Normalization**
Normalization is the process of transforming values of several variables into a similar range. Typical normalizations include scaling the variable so the variable average is 0, scaling the variable so the variance is 1, or scaling variable so the variable values range from 0 to 1

## **Example**
To demonstrate normalization, let's say we want to scale the columns "length", "width" and "height"

- Target:would like to Normalize those variables so their value ranges from 0 to 1.

- Approach: replace original value by (original value)/(maximum value)
```python
# replace (original value) by (original value)/(maximum value)
df['length'] = df['length']/df['length'].max()
df['width'] = df['width']/df['width'].max()
```

```python
df_sub=df[['length', 'width']]
df_sub.tail(20)
```
---
# **Part 4: Uniqueness: Count Values**
## **Example**
```python
# Determine count of unique values for each column in the dataframe
df.nunique()
```
---
# **Part 5: Handle inconsistance data**
Often when cleaning up a dataset, you’ll need to replace or rename inconsistent data. This could be typos, duplicates that are just slightly off, or even a numerical values to string. Your analysis will dictate how (or if) you should replace or rename values.

## **RENAME COLUMN**
It’s good practice to have column names be consistent, lowercase, and no spaces (use underscore instead). Following this practice prevents programming languages from misunderstanding the naming convention and helps you remember the column names later. This particular dataset doesn’t really have column naming issues, but lets say we wanted to rename a column for readability. Use the .rename(columns={'current_column_name':’new_column_name'}) function to rename the column. This method gives you the ability to make your dataset columns readable and consistent.

```python
# Rename a column
df = df.rename(columns={'sex': 'gender'})
```
---
# **Part 6: Identify Missing Values**
```python
#Import Python Libraries
# explicitly require this experimental feature
from sklearn.experimental import enable_iterative_imputer
# now you can import normally from sklearn.impute
from sklearn.impute import IterativeImputer
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.linear_model import BayesianRidge
```

```python
# Read a dataset with missing values
flights = pd.read_csv("flights.csv")
# Use the method <b>head()</b> to display the first five rows of the dataframe. 
flights.head(10)
```
## **Identify_missing_values**

### Evaluating for Missing Data
The missing values are converted to Python's default. We use Python's built-in functions to identify these missing values. There are two methods to detect missing data:

- .isnull()
- .notnull()
The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data.
```python
flights.dtypes
```
```python
# Select the rows that have at least one missing value
missing_data=flights.isnull().any()
missing_data
```
```python
flights.dep_delay.isnull().any()
```
"True" stands for missing value, while "False" stands for not missing value.
---

# **Part 7: Deal with missing data**
- drop data
    a. drop the whole row
    b. drop the whole column
- replace data
    a. replace it by mean
    b. replace it by frequency
    c. replace it based on other functions

Whole columns should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty enough to drop entirely. We have some freedom in choosing which method to replace data; however, some methods may seem more reasonable than others. We will apply each method to many different columns:
## 1. Drop the whole row:  

```python
# Remove all the observations with missing values in any raw
flights_clean = flights.dropna()
```
```python
# check for any remaing missing values
remain_miss=flights_clean.isnull().any()
remain_miss
```

## 2. Drop the whole column::  

```python
# simply drop the air_time column
flights.drop(["air_time"], axis=1, inplace=True)
flights.head()
```
## 3. Replace missing data by mean   
```python
avg_arr_time = flights["arr_time"].astype("float").mean(axis=0)
print("Average arrival time:", avg_arr_time)
```
Replace "NaN" by mean value in "arr_time" column
To see which values are present in a particular column, we can use the ".value_counts()" method:
```python
flights['arr_time'].value_counts()
```
## 4. Replace it by frequency:
```python
flights['dep_delay'].value_counts()
```
We can see that -4 hours are the most common delay. We can also use the ".idxmax()" method to calculate for us the most common occurance of a value automatically:
```python
# This step will print the most frequent value
flights['dep_delay'].value_counts().idxmax()
```
```python
flights["dep_delay"].replace(np.nan, flights['dep_delay'].value_counts().idxmax(), inplace=True)
```
The replacement procedure is very similar to what we have seen previously

```python
#replace the missing 'dep-delay' values by the most frequent 
flights["dep_delay"].replace(np.nan, -4, inplace=True)
# to check the validity of the process
flights["dep_delay"].value_counts()
```



## 5. Using Linear Regression Imputation
```python
# Read a dataset with missing values
flights = pd.read_csv("flights.csv")
# Use the method <b>head()</b> to display the first five rows of the dataframe. 
flights.head(10)
```

```python
# Select the rows that have at least one missing value
missing_data=flights.isnull().any()
missing_data
```
```python
flights['dep_time'].isnull().value_counts()
```
```python
flights['dep_time'].isnull().value_counts()
```

```python
#flights['dep_time'].replace(np.nan, 0, inplace=True)
#flights['arr_time'].replace(np.nan, 0, inplace=True)

#num_missing = (flights[['dep_time','arr_time']] == 0).sum()
#num_missing
```

```python
need_imp=flights[['dep_time','arr_time']]
need_imp.head(10)
```
```python
imp = IterativeImputer(max_iter=10, random_state=0)
imp.fit(need_imp)
imputed=imp.transform(need_imp)
imputed_to_df=pd.DataFrame(imputed)
imputed_to_df.columns=[['dep_time', 'arr_time']]
imputed_to_df
```
```python
# check the final result after imputation
#imputed_to_df.isnull().value_counts()
imputed_to_df.head(10)
```

---
---

# ---------Optional:  More Details----------------

# ------------------------------------------------



### **Expected Outcome**
- The datasets are successfully loaded into Pandas DataFrames.

## **Step 3: Initial Data Exploration**
We inspect the datasets for structure, data types, and missing values.

```python
datasets = {'Auto': auto_df, 'Salaries': salaries_df, 'Flights': flights_df}

for name, df in datasets.items():
    print(f"\n=== {name} Dataset ===")
    print(f"Shape: {df.shape}")
    print(f"Data Types:\n{df.dtypes}")
    print(f"Missing Values:\n{df.isnull().sum()}")
    print(f"Summary Statistics:\n{df.describe(include='all')}")
```

### **Expected Outcome**
- The shape (rows, columns) of each dataset.
- The data types of each column.
- The number of missing values in each column.
- A statistical summary of numerical and categorical features.

## **Step 4: Visualizing Missing Values**
A heatmap is used to visualize missing values.

```python
plt.figure(figsize=(12, 6))
for i, (name, df) in enumerate(datasets.items(), 1):
    plt.subplot(1, 3, i)
    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
    plt.title(f"Missing Values: {name}")
plt.tight_layout()
plt.show()
```

### **How to Read a Heatmap?**
- Darker areas indicate missing values.
- Lighter areas indicate non-missing values.
- Helps in identifying patterns of missing data across datasets.

## **Step 5: Handling Missing Values**
We handle missing values using two strategies:
- **Numerical data**: Fill missing values with the median.
- **Categorical data**: Fill missing values with the most frequent value (mode).

```python
def handle_missing_values(df):
    for col in df.select_dtypes(include=['float64', 'int64']).columns:
        df[col].fillna(df[col].median(), inplace=True)

    for col in df.select_dtypes(include=['object']).columns:
        df[col].fillna(df[col].mode()[0], inplace=True)

    return df

auto_df = handle_missing_values(auto_df)
salaries_df = handle_missing_values(salaries_df)
flights_df = handle_missing_values(flights_df)
```

### **Expected Outcome**
- Missing numerical values are replaced with the column median.
- Missing categorical values are replaced with the most frequent value.

## **Step 6: Detecting and Removing Duplicates**
Duplicate records can bias the analysis. We identify and remove them.

```python
for name, df in datasets.items():
    print(f"\n{name} Dataset:")
    print(f"Duplicate Rows Before: {df.duplicated().sum()}")
    df.drop_duplicates(inplace=True)
    print(f"Duplicate Rows After: {df.duplicated().sum()}")
```

### **Expected Outcome**
- The number of duplicate rows before and after removal.
- Cleaner datasets with unique records.

## **Step 7: Correlation Heatmap**
A heatmap is used to visualize correlations between numerical features.

```python
for name, df in datasets.items():
    plt.figure(figsize=(8, 6))
    sns.heatmap(df.select_dtypes(include=['float64', 'int64']).corr(), annot=True, cmap="coolwarm", fmt=".2f")
    plt.title(f"Correlation Heatmap - {name} Dataset")
    plt.show()
```

### **How to Read a Correlation Heatmap?**
- Values range from **-1 to 1**.
- **1.0**: Perfect positive correlation (one variable increases, the other increases).
- **-1.0**: Perfect negative correlation (one variable increases, the other decreases).
- **0**: No correlation.

For example, in the **auto.csv dataset**:
- If `price` and `horsepower` have a correlation of **0.86**, it means cars with more horsepower tend to be more expensive.
- If `price` and `highway-mpg` have a correlation of **-0.71**, it means higher fuel efficiency tends to lower car prices.

## **Step 8: Export Cleaned Data**
The cleaned datasets are saved for future analysis.

```python
auto_df.to_csv('cleaned_auto.csv', index=False)
salaries_df.to_csv('cleaned_salaries.csv', index=False)
flights_df.to_csv('cleaned_flights.csv', index=False)
print("Cleaned datasets saved successfully!")
```

### **Final Outcome**
- Cleaned versions of the datasets are saved for further use.

## **Conclusion**
In this lab, we covered essential data quality tasks:
- **Data Inspection:** Checking structure, types, and missing values.
- **Handling Missing Values:** Using median for numerical data and mode for categorical data.
- **Removing Duplicates:** Ensuring unique records.
- **Heatmaps:** Understanding correlations between features.

By following this step-by-step approach, students can improve data quality and prepare datasets for effective analysis.


