
# **Data Quality**
- Data Quality refers to the accuracy, completeness, reliability, and relevance of data. It ensures that data is fit for consumption by organizations and decision-makers.

- Data quality is a crucial aspect of data analysis and machine learning. Poor data quality can lead to incorrect analysis, misleading insights, and unreliable decision-making. In this lab, we will walk through a **step-by-step process** to analyze and improve data quality. We will cover:
    - Identifying missing values
    - Handling missing values
    - Detecting and removing duplicates
    - Understanding correlation through heatmaps
    - Standardizing and validating data
## **Importance**
- High-quality data is essential for accurate decision-making and business efficiency.
- Poor data quality leads to errors, inefficiencies, and compliance risks.

## **Key Challenges**
- Lack of standardized definitions and governance.
- Data silos causing inconsistencies.
- Inadequate documentation and training.

**Example:** A financial institution using inconsistent customer records may face challenges in customer support, fraud detection, and compliance monitoring.

---
# **Practice Example: Data Quality Analysis - Step-by-Step Tutorial**
We will use three datasets for this lab: 
- **auto.csv**: Contains vehicle-related data
- **Salaries.csv**: Represents salary information of employees
- **flights.csv**: Contains flight details

## **Step 1: Import Required Libraries**
Before starting, we need to import the necessary Python libraries.

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
print("Libraries imported successfully!")
```

### **Explanation**
- `pandas`: Handles data manipulation and processing.
- `seaborn`: Used for visualization.
- `matplotlib`: Helps in plotting graphs.
- `numpy`: Supports numerical computations.

## **Step 2: Load the Datasets**
We load the three datasets to explore their structure.

```python
auto_df = pd.read_csv("/mnt/data/auto.csv")
salaries_df = pd.read_csv("/mnt/data/Salaries.csv")
flights_df = pd.read_csv("/mnt/data/flights.csv")
print("Datasets loaded successfully!")
```

### **Expected Outcome**
- The datasets are successfully loaded into Pandas DataFrames.

## **Step 3: Initial Data Exploration**
We inspect the datasets for structure, data types, and missing values.

```python
datasets = {'Auto': auto_df, 'Salaries': salaries_df, 'Flights': flights_df}

for name, df in datasets.items():
    print(f"\n=== {name} Dataset ===")
    print(f"Shape: {df.shape}")
    print(f"Data Types:\n{df.dtypes}")
    print(f"Missing Values:\n{df.isnull().sum()}")
    print(f"Summary Statistics:\n{df.describe(include='all')}")
```

### **Expected Outcome**
- The shape (rows, columns) of each dataset.
- The data types of each column.
- The number of missing values in each column.
- A statistical summary of numerical and categorical features.

## **Step 4: Visualizing Missing Values**
A heatmap is used to visualize missing values.

```python
plt.figure(figsize=(12, 6))
for i, (name, df) in enumerate(datasets.items(), 1):
    plt.subplot(1, 3, i)
    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
    plt.title(f"Missing Values: {name}")
plt.tight_layout()
plt.show()
```

### **How to Read a Heatmap?**
- Darker areas indicate missing values.
- Lighter areas indicate non-missing values.
- Helps in identifying patterns of missing data across datasets.

## **Step 5: Handling Missing Values**
We handle missing values using two strategies:
- **Numerical data**: Fill missing values with the median.
- **Categorical data**: Fill missing values with the most frequent value (mode).

```python
def handle_missing_values(df):
    for col in df.select_dtypes(include=['float64', 'int64']).columns:
        df[col].fillna(df[col].median(), inplace=True)

    for col in df.select_dtypes(include=['object']).columns:
        df[col].fillna(df[col].mode()[0], inplace=True)

    return df

auto_df = handle_missing_values(auto_df)
salaries_df = handle_missing_values(salaries_df)
flights_df = handle_missing_values(flights_df)
```

### **Expected Outcome**
- Missing numerical values are replaced with the column median.
- Missing categorical values are replaced with the most frequent value.

## **Step 6: Detecting and Removing Duplicates**
Duplicate records can bias the analysis. We identify and remove them.

```python
for name, df in datasets.items():
    print(f"\n{name} Dataset:")
    print(f"Duplicate Rows Before: {df.duplicated().sum()}")
    df.drop_duplicates(inplace=True)
    print(f"Duplicate Rows After: {df.duplicated().sum()}")
```

### **Expected Outcome**
- The number of duplicate rows before and after removal.
- Cleaner datasets with unique records.

## **Step 7: Correlation Heatmap**
A heatmap is used to visualize correlations between numerical features.

```python
for name, df in datasets.items():
    plt.figure(figsize=(8, 6))
    sns.heatmap(df.select_dtypes(include=['float64', 'int64']).corr(), annot=True, cmap="coolwarm", fmt=".2f")
    plt.title(f"Correlation Heatmap - {name} Dataset")
    plt.show()
```

### **How to Read a Correlation Heatmap?**
- Values range from **-1 to 1**.
- **1.0**: Perfect positive correlation (one variable increases, the other increases).
- **-1.0**: Perfect negative correlation (one variable increases, the other decreases).
- **0**: No correlation.

For example, in the **auto.csv dataset**:
- If `price` and `horsepower` have a correlation of **0.86**, it means cars with more horsepower tend to be more expensive.
- If `price` and `highway-mpg` have a correlation of **-0.71**, it means higher fuel efficiency tends to lower car prices.

## **Step 8: Export Cleaned Data**
The cleaned datasets are saved for future analysis.

```python
auto_df.to_csv('cleaned_auto.csv', index=False)
salaries_df.to_csv('cleaned_salaries.csv', index=False)
flights_df.to_csv('cleaned_flights.csv', index=False)
print("Cleaned datasets saved successfully!")
```

### **Final Outcome**
- Cleaned versions of the datasets are saved for further use.

## **Conclusion**
In this lab, we covered essential data quality tasks:
- **Data Inspection:** Checking structure, types, and missing values.
- **Handling Missing Values:** Using median for numerical data and mode for categorical data.
- **Removing Duplicates:** Ensuring unique records.
- **Heatmaps:** Understanding correlations between features.

By following this step-by-step approach, students can improve data quality and prepare datasets for effective analysis.


